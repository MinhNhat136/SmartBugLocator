{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d78a8b75eaea1f5",
   "metadata": {},
   "source": "## PREPROCESSING DATA"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:15.315710Z",
     "start_time": "2025-04-12T08:41:14.509973Z"
    }
   },
   "source": [
    "from mpmath.libmp import normalize\n",
    "\n",
    "from preprocessing import Parser, ReportPreprocessor, SrcPreprocessor\n",
    "from datasets import DATASETS\n",
    "\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "340c1372-80a5-4286-b398-6300e04e7e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:15.642463Z",
     "start_time": "2025-04-12T08:41:15.328758Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nhatm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\nhatm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "017deb6b-c635-4cf6-a8b3-bbc1c77feb6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:49:34.820232Z",
     "start_time": "2025-04-11T09:49:34.817650Z"
    }
   },
   "source": [
    "def preprocessing_data(dataset):\n",
    "    parser = Parser(dataset)\n",
    "    bug_reports = parser.report_parser()\n",
    "    src_files = parser.src_parser()\n",
    "\n",
    "    os.makedirs(f'outputs/{dataset.name}', exist_ok=True)\n",
    "\n",
    "    SrcPreprocessor(src_files).preprocess_and_export(dataset.name)\n",
    "    ReportPreprocessor(bug_reports).preprocess_and_export(dataset.name)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-11T09:49:34.844120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for DATASET in DATASETS:\n",
    "    preprocessing_data(DATASET)"
   ],
   "id": "5bf5275c44d766ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## INDEXING SOURCE CODE FILES\n",
   "id": "fdcd0c9ad3294b33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:16.644753Z",
     "start_time": "2025-04-12T08:41:16.642441Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_name = 'tomcat'",
   "id": "808aea35aed54545",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "ff7344416bf756c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:17.313537Z",
     "start_time": "2025-04-12T08:41:16.997716Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "be114cc0a930d52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:17.386160Z",
     "start_time": "2025-04-12T08:41:17.382161Z"
    }
   },
   "source": [
    "def prepare_dataframe_src_code(name):\n",
    "    return pd.read_csv(f\"outputs/{name}/source_code_data.csv\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:17.911212Z",
     "start_time": "2025-04-12T08:41:17.907044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fix_and_fetch_src_code_infor(data_src_code):\n",
    "    def extract_stemmed(column, field = 'stemmed'):\n",
    "        return column.apply(ast.literal_eval).apply(lambda x: x[field])\n",
    "\n",
    "    stem_columns = ['pos_tagged_comments']\n",
    "    for col in stem_columns:\n",
    "        data_src_code[col] = extract_stemmed(data_src_code[col])\n",
    "\n",
    "    un_stem_columns = ['file_name', 'class_names', 'method_names']\n",
    "    for col in un_stem_columns:\n",
    "        data_src_code[col] = extract_stemmed(data_src_code[col], 'unstemmed')\n",
    "\n",
    "    data_src_code['natural_language'] = data_src_code['pos_tagged_comments']\n",
    "    data_src_code['code_entities'] = data_src_code.apply(lambda row: row['file_name'] + row['class_names']+ row['method_names'], axis=1)\n",
    "\n",
    "    return data_src_code[['key', 'natural_language', 'code_entities', 'total_lines']]\n"
   ],
   "id": "384cb968119fadaf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "289d819f926efbd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:18.378292Z",
     "start_time": "2025-04-12T08:41:18.375098Z"
    }
   },
   "source": [
    "def build_inverted_index(dataset, column_data_string):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for data in dataset.iterrows():\n",
    "        for content in data[1][column_data_string]:\n",
    "            inverted_index[content].add(data[1]['key'])\n",
    "    return inverted_index"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:20.282323Z",
     "start_time": "2025-04-12T08:41:18.882652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_dataset = fix_and_fetch_src_code_infor(prepare_dataframe_src_code(dataset_name))\n",
    "inverted_index_natural_language_src_codes = build_inverted_index(df_dataset, 'natural_language')\n",
    "inverted_index_code_entities_src_codes = build_inverted_index(df_dataset, 'code_entities')"
   ],
   "id": "d6cfe3145242a041",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "322f41dfda205c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:20.840828Z",
     "start_time": "2025-04-12T08:41:20.836309Z"
    }
   },
   "source": [
    "def compute_ltc(documents, inverted_index, field):\n",
    "    tf_idf = defaultdict(lambda: defaultdict(float))\n",
    "    total_documents = len(documents)\n",
    "\n",
    "    for data in documents.iterrows():\n",
    "        doc_id = data[1].key\n",
    "        field_content = data[1][field]\n",
    "\n",
    "        content_frequency = defaultdict(int)\n",
    "\n",
    "        if isinstance(field_content, list) and len(field_content) > 0:\n",
    "            for content in field_content:\n",
    "                content_frequency[content] += 1\n",
    "\n",
    "            for content, count in content_frequency.items():\n",
    "                tf = 1 + math.log(count, 10)\n",
    "                df = len(inverted_index.get(content, []))\n",
    "                idf = math.log(total_documents / df, 10) if df != 0 else 0\n",
    "\n",
    "                tf_idf[doc_id][content] = tf * idf\n",
    "        else:\n",
    "            tf_idf[doc_id] = {}\n",
    "\n",
    "    for doc_id in tf_idf:\n",
    "        norm = math.sqrt(sum(weight ** 2 for weight in tf_idf[doc_id].values()))\n",
    "        if norm > 0:\n",
    "            for content in tf_idf[doc_id]:\n",
    "                tf_idf[doc_id][content] /= norm\n",
    "\n",
    "    return tf_idf\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "caefb66e5980cba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:21.588010Z",
     "start_time": "2025-04-12T08:41:21.442244Z"
    }
   },
   "source": [
    "natural_lang_vsm_src_codes = compute_ltc(df_dataset, inverted_index_natural_language_src_codes, 'natural_language')\n",
    "code_entities_vsm_src_codes = compute_ltc(df_dataset, inverted_index_code_entities_src_codes, 'code_entities')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "6bd66b7a09944bd2",
   "metadata": {},
   "source": [
    "## CALCULATE COEFFICIENT FILE SIZE"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:22.712042Z",
     "start_time": "2025-04-12T08:41:22.707477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_size_dataset = (df_dataset.total_lines.min(), df_dataset.total_lines.max())\n",
    "min_max_size_dataset"
   ],
   "id": "97f946a86ea261eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 138)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a3a53ec93e31827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:23.263363Z",
     "start_time": "2025-04-12T08:41:23.259853Z"
    }
   },
   "source": [
    "def calculate_coefficient_size(size, size_data):\n",
    "    min_size, max_size = size_data\n",
    "    value_standardization = (size - min_size) / (max_size - min_size)\n",
    "    return 1/(1 + math.exp(-value_standardization))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:23.718797Z",
     "start_time": "2025-04-12T08:41:23.715474Z"
    }
   },
   "cell_type": "code",
   "source": "df_dataset['coefficient_size'] = df_dataset['total_lines'].apply(lambda x: calculate_coefficient_size(x, min_max_size_dataset))",
   "id": "c4b9abb4577c65ad",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "27ec623e3368f0d",
   "metadata": {},
   "source": "## INDEXING BUG REPORTS"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:23:48.182788Z",
     "start_time": "2025-04-12T12:23:48.177219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_dataframe_bug_reports(name):\n",
    "    return pd.read_csv(f\"outputs/{name}/bug_reports.csv\")"
   ],
   "id": "e0548480f127ac3d",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:23:48.580302Z",
     "start_time": "2025-04-12T12:23:48.575903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fix_and_fetch_bug_report(report):\n",
    "    report.pos_tagged_description = report.pos_tagged_description.apply(ast.literal_eval)\n",
    "    report.pos_tagged_description = report.pos_tagged_description.apply(lambda x: x['unstemmed'])\n",
    "\n",
    "    report.pos_tagged_summary = report.pos_tagged_summary.apply(ast.literal_eval)\n",
    "    report.pos_tagged_summary = report.pos_tagged_summary.apply(lambda x: x['stemmed'])\n",
    "\n",
    "    report['fixed_files'] = report['fixed_files'].apply(lambda x: [f for f in x.split() if f != '.'] if isinstance(x, str) else x)\n",
    "\n",
    "    report['natural_language'] = report.pos_tagged_summary\n",
    "    report['code_entities'] = report.pos_tagged_description\n",
    "\n",
    "    return report[['key', 'natural_language', 'code_entities', 'report_time', 'fixed_files']]"
   ],
   "id": "32e07b0c666bd3af",
   "outputs": [],
   "execution_count": 219
  },
  {
   "cell_type": "code",
   "id": "14bc28e2d90c1b10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:45:32.500863Z",
     "start_time": "2025-04-12T12:45:32.495606Z"
    }
   },
   "source": [
    "def compute_lnc(query, field):\n",
    "    tf_idf = defaultdict(float)\n",
    "\n",
    "    term_freq = defaultdict(int)\n",
    "    for term in query[field]:\n",
    "        term_freq[term] += 1\n",
    "\n",
    "    for term, freq in term_freq.items():\n",
    "        tf = 1 + math.log10(freq)\n",
    "        tf_idf[term] = tf * 1\n",
    "\n",
    "    norm = math.sqrt(sum(weight ** 2 for weight in tf_idf.values()))\n",
    "    if norm > 0:\n",
    "        for term in tf_idf:\n",
    "            tf_idf[term] /= norm\n",
    "\n",
    "    return tf_idf"
   ],
   "outputs": [],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:23:50.219021Z",
     "start_time": "2025-04-12T12:23:50.215426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_bnc(query, field):\n",
    "    tf_idf = defaultdict(float)\n",
    "\n",
    "    unique_terms = set(query[field])\n",
    "\n",
    "    for term in unique_terms:\n",
    "        tf = 1\n",
    "        tf_idf[term] = tf * 1\n",
    "\n",
    "    norm = math.sqrt(sum(weight ** 2 for weight in tf_idf.values()))\n",
    "    if norm > 0:\n",
    "        for term in tf_idf:\n",
    "            tf_idf[term] /= norm\n",
    "\n",
    "    return tf_idf\n"
   ],
   "id": "57a4f63e281abc2f",
   "outputs": [],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:23:51.427344Z",
     "start_time": "2025-04-12T12:23:51.239787Z"
    }
   },
   "cell_type": "code",
   "source": "bug_report_dataset = fix_and_fetch_bug_report(prepare_dataframe_bug_reports(dataset_name))",
   "id": "ac8a2055a1a60292",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CALCULATE rVSM",
   "id": "4300300ecb1d88bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:28.382951Z",
     "start_time": "2025-04-12T08:41:28.380031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_indexing_score(query, vsm, compute_query, field):\n",
    "    query_vec = compute_query(query, field)\n",
    "    indexing_scores = {}\n",
    "\n",
    "    for directory, doc_vec in vsm.items():\n",
    "        dot_product = 0.0\n",
    "        for term in query_vec:\n",
    "            dot_product += query_vec[term] * doc_vec.get(term, 0.0)\n",
    "\n",
    "        indexing_scores[directory] = dot_product * df_dataset.loc[df_dataset['key'] == directory, 'coefficient_size'].iloc[0]\n",
    "\n",
    "    return indexing_scores"
   ],
   "id": "829b5c0af3396404",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:28.929868Z",
     "start_time": "2025-04-12T08:41:28.926354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_rank_files_indexing(query, coefficient):\n",
    "    scores_indexing = defaultdict(float)\n",
    "    scores_nl = compute_indexing_score(query, natural_lang_vsm_src_codes, compute_lnc,'natural_language')\n",
    "    scores_ce = compute_indexing_score(query, code_entities_vsm_src_codes, compute_lnc,'code_entities')\n",
    "\n",
    "    for score_key in scores_ce:\n",
    "        scores_indexing[score_key] = coefficient * scores_nl[score_key] + (1 - coefficient)* scores_ce[score_key]\n",
    "\n",
    "    return scores_indexing"
   ],
   "id": "b15c7f93376f757e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GET PREVIOUS BUG REPORTS",
   "id": "79a11c4267d84ccd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:30.482444Z",
     "start_time": "2025-04-12T08:41:30.479279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_previous_bug_fixed_report(df, current_bug_id):\n",
    "    df_sorted = df.sort_values(by='report_time')\n",
    "    previous_bugs = df_sorted[df_sorted['report_time'] < df_sorted[df_sorted['key'] == current_bug_id]['report_time'].iloc[0]]\n",
    "    return previous_bugs\n",
    "\n",
    "def calculate_bug_query_similarity(df, current_bug_id):\n",
    "    previous_bugs = get_previous_bug_fixed_report(df, current_bug_id)\n",
    "    inverted_index = build_inverted_index(previous_bugs, 'natural_language')\n",
    "    pre_bug_reports_vector = compute_ltc(previous_bugs, inverted_index, 'natural_language')\n",
    "\n",
    "    current_bug_vector = compute_lnc(df[df.key == current_bug_id].iloc[0], 'natural_language')\n",
    "\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for bug_id, words in pre_bug_reports_vector.items():\n",
    "        dot_product = 0.0\n",
    "        for term in current_bug_vector:\n",
    "            dot_product += current_bug_vector[term] * words.get(term, 0.0)\n",
    "\n",
    "        similarity_scores[bug_id] = dot_product\n",
    "\n",
    "    return similarity_scores"
   ],
   "id": "d1fda2c35f2f646d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:31.196503Z",
     "start_time": "2025-04-12T08:41:31.191995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_related_files(df, similarity_dict):\n",
    "    relevant_bugs = {bug_id: score for bug_id, score in similarity_dict.items()}\n",
    "\n",
    "    if not relevant_bugs:\n",
    "        return {}\n",
    "\n",
    "    file_scores = {}\n",
    "\n",
    "    for bug_id, score in relevant_bugs.items():\n",
    "        fixed_files = df[df['key'] == bug_id]['fixed_files'].iloc[0]\n",
    "        for fixed_file in fixed_files:\n",
    "            if fixed_file in file_scores:\n",
    "                file_scores[fixed_file].append(score)\n",
    "            else:\n",
    "                file_scores[fixed_file] = [score]\n",
    "\n",
    "    average_file_scores = {fixed_file: sum(fixed_file_scores) / len(fixed_file_scores) for fixed_file, fixed_file_scores in file_scores.items()}\n",
    "\n",
    "    return average_file_scores"
   ],
   "id": "4977bb7a59d71b93",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CO-CHANGE MATRIX",
   "id": "c1a7a5023fc63ada"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:32.483604Z",
     "start_time": "2025-04-12T08:41:32.174357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for files in bug_report_dataset['fixed_files'].dropna():\n",
    "    if len(files) < 2:\n",
    "        continue\n",
    "    for i in range(len(files)):\n",
    "        for j in range(i + 1, len(files)):\n",
    "            G.add_edge(files[i], files[j])\n",
    "\n",
    "clusters = list(nx.connected_components(G))\n"
   ],
   "id": "95a1dbee636a3d58",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:33.729588Z",
     "start_time": "2025-04-12T08:41:33.726388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_to_cluster = {}\n",
    "for cid, cluster in enumerate(clusters):\n",
    "    for f_name in cluster:\n",
    "        file_to_cluster[f_name] = cid"
   ],
   "id": "bac56cc6dc13b9ea",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FLOW",
   "id": "569f11dd7a1e95f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:41:34.581261Z",
     "start_time": "2025-04-12T08:41:34.577373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_final_score(report, alpha, top_k):\n",
    "    rvsm_score = compute_rank_files_indexing(report, 0.4)\n",
    "\n",
    "    similarity_bug_scores = calculate_bug_query_similarity(bug_report_dataset, report.key)\n",
    "    simi_previous_bugs_score = get_related_files(bug_report_dataset, similarity_bug_scores)\n",
    "\n",
    "    final_score = rvsm_score\n",
    "\n",
    "    for directory in simi_previous_bugs_score:\n",
    "        if directory in final_score:\n",
    "            final_score[directory] = (1 - alpha) * final_score[directory] + alpha * simi_previous_bugs_score[directory]\n",
    "\n",
    "    return dict(sorted(final_score.items(), key=lambda item: item[1], reverse=True)[:top_k])"
   ],
   "id": "a5ffcc6d20e7e5b9",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:49:44.353511Z",
     "start_time": "2025-04-12T08:41:35.076878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "list_prediction = {}\n",
    "\n",
    "for idx, bug_report in bug_report_dataset.iterrows():\n",
    "    scores = calculate_final_score(bug_report, 0.2, 25)\n",
    "\n",
    "    boosted_scores = copy.deepcopy(scores)\n",
    "    cluster_scores = defaultdict(list)\n",
    "\n",
    "    for file in scores:\n",
    "        c_id = file_to_cluster.get(file)\n",
    "        if c_id is not None:\n",
    "            cluster_scores[c_id].append(scores[file])\n",
    "\n",
    "    cluster_avg = {cid: np.mean(scores) for cid, scores in cluster_scores.items()}\n",
    "\n",
    "    for file_name in scores:\n",
    "        c_id = file_to_cluster.get(file_name)\n",
    "        if c_id is not None:\n",
    "            boosted_scores[file_name] += 0.3 * cluster_avg[c_id]\n",
    "\n",
    "    list_prediction[bug_report.key] = boosted_scores"
   ],
   "id": "d31358c6fea6a8ad",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## EVALUATE",
   "id": "759ed7ced6a63fb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:49:48.063143Z",
     "start_time": "2025-04-12T08:49:48.059838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fetch_ground_truth():\n",
    "    ground_truth = {}\n",
    "    for report_data in bug_report_dataset.itertuples():\n",
    "        ground_truth[report_data.key] = report_data.fixed_files\n",
    "    return ground_truth"
   ],
   "id": "a348d4224f28af08",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:49:48.606048Z",
     "start_time": "2025-04-12T08:49:48.600405Z"
    }
   },
   "cell_type": "code",
   "source": "ground_truth_data = fetch_ground_truth()",
   "id": "c71628ba4a39a921",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:49:49.579912Z",
     "start_time": "2025-04-12T08:49:49.576475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def top_k_accuracy(predictions, ground_truth, k=1):\n",
    "    correct = 0\n",
    "    total_queries = len(predictions)\n",
    "\n",
    "    for bug_id in predictions:\n",
    "        temp_prediction = dict(list(predictions[bug_id].items())[:k])\n",
    "        predict_top_k = temp_prediction.keys()\n",
    "        true_directory = ground_truth[bug_id]\n",
    "\n",
    "        if any(doc in true_directory for doc in predict_top_k):\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total_queries if total_queries > 0 else 0"
   ],
   "id": "e1984cc04f02a3c6",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:49:50.735573Z",
     "start_time": "2025-04-12T08:49:50.721051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top1_acc = top_k_accuracy(list_prediction, ground_truth_data, k=1)\n",
    "top5_acc = top_k_accuracy(list_prediction, ground_truth_data, k=5)\n",
    "top10_acc = top_k_accuracy(list_prediction, ground_truth_data, k=10)\n",
    "top20_acc = top_k_accuracy(list_prediction, ground_truth_data, k=20)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {top10_acc:.4f}\")"
   ],
   "id": "bd560f76ee93a94a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.1903\n",
      "Top-5 Accuracy: 0.3892\n",
      "Top-10 Accuracy: 0.4972\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:27.900945Z",
     "start_time": "2025-04-12T10:33:27.897039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_100_predict = dict(list(list_prediction.items())[:10])\n",
    "first_100_ground_truth = dict(list(ground_truth_data.items())[:10])\n",
    "top1_acc = top_k_accuracy(first_100_predict, first_100_ground_truth, k=1)\n",
    "top5_acc = top_k_accuracy(first_100_predict, first_100_ground_truth, k=5)\n",
    "top10_acc = top_k_accuracy(first_100_predict, first_100_ground_truth, k=10)"
   ],
   "id": "bc0e22a4ffc77238",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:28.253358Z",
     "start_time": "2025-04-12T10:33:28.250356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {top10_acc:.4f}\")"
   ],
   "id": "65b1b140b850eb48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.0000\n",
      "Top-5 Accuracy: 0.1000\n",
      "Top-10 Accuracy: 0.3000\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CHUNKS FILE-CODE",
   "id": "7e2d09bcf450c30e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:29.103413Z",
     "start_time": "2025-04-12T10:33:29.100566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import javalang\n",
    "import os\n",
    "import json"
   ],
   "id": "dd601eed0330a5ba",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:29.334329Z",
     "start_time": "2025-04-12T10:33:29.331828Z"
    }
   },
   "cell_type": "code",
   "source": "root_dir_code = 'source_files\\\\tomcat-7.0.51'",
   "id": "6a3a6caafd0975da",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:29.457437Z",
     "start_time": "2025-04-12T10:33:29.454563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_java_file(filepath, root = root_dir_code):\n",
    "    file_path = os.path.join(root, filepath)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors= 'ignore') as java_file:\n",
    "            content = java_file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(filepath)\n",
    "        return \"\""
   ],
   "id": "e516d4b1e0bae862",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:29.604969Z",
     "start_time": "2025-04-12T10:33:29.600628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def segment_java_code(java_code):\n",
    "    tree = javalang.parse.parse(java_code)\n",
    "    lines = java_code.splitlines()\n",
    "    segments = []\n",
    "\n",
    "    for type in tree.types:\n",
    "        for member in type.body:\n",
    "            if not hasattr(member, 'position') or member.position is None:\n",
    "                continue\n",
    "\n",
    "            start_line = member.position.line - 1\n",
    "\n",
    "            open_braces = 0\n",
    "            end_line = start_line\n",
    "            for i in range(start_line, len(lines)):\n",
    "                open_braces += lines[i].count('{')\n",
    "                open_braces -= lines[i].count('}')\n",
    "                if open_braces == 0 and '{' in lines[start_line]:\n",
    "                    end_line = i\n",
    "                    break\n",
    "\n",
    "            code_segment = \"\\n\".join(lines[start_line:end_line + 1])\n",
    "\n",
    "            if isinstance(member, javalang.tree.MethodDeclaration):\n",
    "                segments.append((\"method\", member.name, code_segment))\n",
    "\n",
    "\n",
    "    return segments"
   ],
   "id": "2647c81999177bc4",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:29.761777Z",
     "start_time": "2025-04-12T10:33:29.757776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logic_signals = [\n",
    "    'if', 'else', 'switch', 'case', 'for', 'while', 'do',\n",
    "\n",
    "    'try', 'catch', 'finally', 'throw', 'assert',\n",
    "    'IllegalArgumentException', 'NullPointerException', 'RuntimeException', 'IOException', 'SQLException',\n",
    "\n",
    "    'log', 'logger', 'System.out', 'System.err', 'printStackTrace', 'debug', 'warn', 'trace',\n",
    "\n",
    "    'File', 'FileReader', 'BufferedReader', 'InputStream', 'OutputStream',\n",
    "    'Socket', 'HttpClient', 'URLConnection', 'request', 'response',\n",
    "\n",
    "    '==', '!=', '<=', '>=', '&&', '||',\n",
    "\n",
    "    'null', 'isEmpty', 'isBlank', 'validate', 'check', 'require', 'assertNotNull',\n",
    "\n",
    "    'synchronized', 'Thread', 'Runnable', 'lock', 'unlock', 'wait', 'notify', 'Future',\n",
    "\n",
    "    'auth', 'authenticate', 'token', 'csrf', 'secure', 'session', 'encrypt', 'decrypt'\n",
    "]\n",
    "\n",
    "def has_logic_signals(code_segment):\n",
    "    return any(sig in code_segment for sig in logic_signals)"
   ],
   "id": "cfe5b9d147effa02",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:29.904289Z",
     "start_time": "2025-04-12T10:33:29.901337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_semicolons(code_segment):\n",
    "    return code_segment.count(';') >= 3"
   ],
   "id": "39e09a85560188aa",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:30.578824Z",
     "start_time": "2025-04-12T10:33:30.574444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def group_short_chunks(chunks, max_lines=40, max_group_size=3):\n",
    "    grouped_chunks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(chunks):\n",
    "        current_group = []\n",
    "        total_lines = 0\n",
    "        count = 0\n",
    "\n",
    "        if len(chunks[i].splitlines()) >= max_lines:\n",
    "            grouped_chunks.append(chunks[i])\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        while i < len(chunks) and count < max_group_size:\n",
    "            lines = chunks[i].splitlines()\n",
    "            if total_lines + len(lines) > max_lines:\n",
    "                break\n",
    "            current_group.append(chunks[i])\n",
    "            total_lines += len(lines)\n",
    "            count += 1\n",
    "            i += 1\n",
    "\n",
    "        combined = \"\\n\\n\".join(current_group)\n",
    "        grouped_chunks.append(combined)\n",
    "\n",
    "    return grouped_chunks"
   ],
   "id": "dd5eee4104911df3",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:31.042549Z",
     "start_time": "2025-04-12T10:33:31.039199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "buggy_symptoms = ['fail', 'error', 'invalid', 'exception', 'unauthorized', '403', 'throw new']\n",
    "\n",
    "def mentions_buggy_symptoms(code_segment):\n",
    "    return any(sym in code_segment.lower() for sym in buggy_symptoms)"
   ],
   "id": "4a0591cbfba632de",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:31.447381Z",
     "start_time": "2025-04-12T10:33:31.443877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "skip_exact_names = ['toString', 'hashCode', 'equals', 'main']\n",
    "def is_safe_name(name):\n",
    "    return name in skip_exact_names or name.startswith('get') or name.startswith('set')"
   ],
   "id": "7f55614e21494df7",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:31.852095Z",
     "start_time": "2025-04-12T10:33:31.848916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_logic_signals(code_segment):\n",
    "    return sum(code_segment.lower().count(sig.lower()) for sig in logic_signals)"
   ],
   "id": "e1803523c770e994",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:32.222601Z",
     "start_time": "2025-04-12T10:33:32.218589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_strongly_suspicious_segment(name, code_segment, min_lines=5):\n",
    "    if is_safe_name(name):\n",
    "        return False\n",
    "    lines = code_segment.strip().splitlines()\n",
    "    vote = 0\n",
    "\n",
    "    if len(lines) >= min_lines:\n",
    "        vote += 1\n",
    "    if count_logic_signals(code_segment) >= 3:\n",
    "        vote += 1\n",
    "    if mentions_buggy_symptoms(code_segment):\n",
    "        vote += 1\n",
    "\n",
    "    return vote >= 2\n"
   ],
   "id": "5b6a73489228e1ac",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:32.705254Z",
     "start_time": "2025-04-12T10:33:32.702088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_predicted_result_chunks(directories):\n",
    "    dict_chunks = {}\n",
    "\n",
    "    for directory in directories:\n",
    "        java_file = read_java_file(directory)\n",
    "        chunks = segment_java_code(java_file)\n",
    "        filtered_chunks = [code for seg_type, name, code in chunks if is_strongly_suspicious_segment(name, code)]\n",
    "        if len(filtered_chunks) == 0:\n",
    "            continue\n",
    "        combined_filtered_chunks = group_short_chunks(filtered_chunks)\n",
    "        dict_chunks[directory] = combined_filtered_chunks\n",
    "\n",
    "    return dict_chunks"
   ],
   "id": "659d70ab3099d140",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:35.027312Z",
     "start_time": "2025-04-12T10:33:33.163211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_list_predicted_results_chunks(predictions):\n",
    "    code_dict_chunks = {}\n",
    "\n",
    "    for index in predictions:\n",
    "        chunks = build_predicted_result_chunks(predictions[index].keys())\n",
    "        code_dict_chunks[index] = chunks\n",
    "\n",
    "    return code_dict_chunks\n",
    "\n",
    "predicted_report_chunks = build_list_predicted_results_chunks(first_100_predict)"
   ],
   "id": "c30c391aa9dfb2e3",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:33:35.032598Z",
     "start_time": "2025-04-12T10:33:35.029599Z"
    }
   },
   "cell_type": "code",
   "source": "pd.set_option('display.max_colwidth', None)",
   "id": "951ef5d3ca175d42",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/Tomcat.txt', sep='\\t')\n",
    "bug_summary_df = df[['bug_id', 'summary']]\n",
    "bug_summary_df.set_index('bug_id')"
   ],
   "id": "863f777bf7b3f42f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:34:02.670810Z",
     "start_time": "2025-04-12T10:34:02.664074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file = {}\n",
    "for bug_id in predicted_report_chunks:\n",
    "    summary = bug_summary_df[bug_summary_df.bug_id == bug_id]['summary'].to_string(index=False).strip()\n",
    "    output_file[bug_id] = {'summary': summary, 'code_segments': predicted_report_chunks[bug_id]}"
   ],
   "id": "577e10c07bfd44f3",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:34:04.736227Z",
     "start_time": "2025-04-12T10:34:04.725219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('output.json', 'w') as json_file:\n",
    "    json.dump(output_file, json_file, indent=4)"
   ],
   "id": "63e130e90cb867bb",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LLM EVALUATE",
   "id": "1aeac962f7f50066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:34:08.868138Z",
     "start_time": "2025-04-12T10:34:08.705873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")"
   ],
   "id": "8afbe0dedc06d674",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:34:10.164009Z",
     "start_time": "2025-04-12T10:34:10.161198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_message = \"\"\"You are a helpful AI software engineer specializing in identifying buggy code segments given a bug report. Analyze the provided bug report and the JAVA code segment to determine if the code segment is responsible for causing the bug described in the bug report. You need to understand the functionality of the code segment and the details of the bug report to determine the relevance of the code segment to the bug report. There are two possible outputs: 'yes', 'no'.\n",
    "'yes': The code is responsible for the bug described in the bug report.\n",
    "'no': The code is NOT responsible for the bug described in the bug report.\n",
    "Provide your output in JSON format like this sample: {{\"relevance\": \"yes\"}}.\n",
    "Act like a rational software engineer and provide output. Avoid emotion and extra text other than JSON.\"\"\"\n",
    "\n",
    "def prompt_engineering(report_query, code_segment):\n",
    "    return f\"\"\"Analyze the following bug report and code segment:\n",
    "    Bug Report: {report_query}\n",
    "    Code Segment: {code_segment}\n",
    "    Please determine if the code segment is responsible for the bug described in the bug report.\"\"\""
   ],
   "id": "c1d33e4b0e49e3af",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:34:11.079555Z",
     "start_time": "2025-04-12T10:34:11.076913Z"
    }
   },
   "cell_type": "code",
   "source": "import re",
   "id": "c53755d0d793b133",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:35:24.648102Z",
     "start_time": "2025-04-12T10:35:24.645150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_llm(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"anthropic/claude-3.5-haiku:beta\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = getattr(completion.choices[0], 'message', {}).get('content') if isinstance(completion.choices[0].message, dict) else completion.choices[0].message.content\n",
    "\n",
    "    if content is None:\n",
    "        print(\"No content in LLM response.\")\n",
    "        return 'no'\n",
    "\n",
    "    # Regex check\n",
    "    match = re.search(r'\"relevance\"\\s*:\\s*\"(\\w+)\"', content)\n",
    "    if match:\n",
    "        relevance = match.group(1)\n",
    "        print('match result:', relevance)\n",
    "        return relevance\n",
    "\n",
    "    return 'no'\n"
   ],
   "id": "9faba15ff086efc2",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T10:35:29.930953Z",
     "start_time": "2025-04-12T10:35:29.927123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def llm_evaluation():\n",
    "    output_relevance_segments = defaultdict(list)\n",
    "    for bug_id in output_file:\n",
    "        for directory in output_file[bug_id]['code_segments']:\n",
    "            for i in range(len(output_file[bug_id]['code_segments'][directory])):\n",
    "                relevance = chat_llm(prompt_engineering(output_file[bug_id]['summary'], output_file[bug_id]['code_segments'][directory][i]))\n",
    "                if relevance == 'yes':\n",
    "                    output_relevance_segments[bug_id].append(output_file[bug_id]['code_segments'][directory][i])\n",
    "    return output_relevance_segments"
   ],
   "id": "9d820499c7437940",
   "outputs": [],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "relevance_segments = llm_evaluation()",
   "id": "3d561a1216aff1fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T11:36:55.833349Z",
     "start_time": "2025-04-12T11:36:55.827442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('output_segment.json', 'w') as json_file:\n",
    "    json.dump(relevance_segments, json_file, indent=4)"
   ],
   "id": "40c2acf22e4e89dd",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## QUERY EXPANSION",
   "id": "b1ec78c31d743f49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:58:01.454073Z",
     "start_time": "2025-04-12T12:58:01.448786Z"
    }
   },
   "cell_type": "code",
   "source": "list_10_bug_report_dataset = bug_report_dataset[:10]",
   "id": "cba5938a3fc19b8d",
   "outputs": [],
   "execution_count": 297
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:58:02.131272Z",
     "start_time": "2025-04-12T12:58:02.127058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_code_entities(code_string):\n",
    "    entities = set()\n",
    "\n",
    "    method_calls = re.findall(r'\\b([a-zA-Z_][a-zA-Z0-9_]*)\\s*\\(', code_string)\n",
    "    entities.update(method_calls)\n",
    "\n",
    "    variable_defs = re.findall(r'\\b(?:String|int|boolean|Throwable)\\s+([a-zA-Z_][a-zA-Z0-9_]*)', code_string)\n",
    "    entities.update(variable_defs)\n",
    "\n",
    "    param_types = re.findall(r'\\b([A-Z][a-zA-Z0-9_]*)\\s+[a-zA-Z_][a-zA-Z0-9_]*', code_string)\n",
    "    entities.update(param_types)\n",
    "\n",
    "    return list(sorted(entities))"
   ],
   "id": "30191d3b8bf45ae",
   "outputs": [],
   "execution_count": 298
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:58:05.337065Z",
     "start_time": "2025-04-12T12:58:05.332450Z"
    }
   },
   "cell_type": "code",
   "source": "from assets import java_keywords",
   "id": "c6c0eff2759215ae",
   "outputs": [],
   "execution_count": 299
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:58:05.759015Z",
     "start_time": "2025-04-12T12:58:05.743062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extracted_code_entities = defaultdict(list)\n",
    "for bug_id in relevance_segments:\n",
    "    for code_segment_index in range(len(relevance_segments[bug_id])):\n",
    "        code_entities = [code_entity for code_entity in extract_code_entities(relevance_segments[bug_id][code_segment_index]) if code_entity not in java_keywords]\n",
    "        extracted_code_entities[bug_id].append(code_entities)\n",
    "\n",
    "for bug_id in extracted_code_entities:\n",
    "    merged_code_entities = []\n",
    "    for list_code_entities in extracted_code_entities[bug_id]:\n",
    "        merged_code_entities.extend(list_code_entities)\n",
    "\n",
    "    extracted_code_entities[bug_id] = merged_code_entities"
   ],
   "id": "3cf282bbf7d1206b",
   "outputs": [],
   "execution_count": 300
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:02:16.580526Z",
     "start_time": "2025-04-12T13:02:16.574839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def append_code_entities(row):\n",
    "    key = row['key']\n",
    "    if key in extracted_code_entities:\n",
    "        existing = row['code_entities']\n",
    "        print(existing)\n",
    "        if not existing:\n",
    "            existing = []\n",
    "        return existing + extracted_code_entities[key]\n",
    "    return row['code_entities']"
   ],
   "id": "7a8ba26ec4ccdb79",
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:14:37.230815Z",
     "start_time": "2025-04-12T13:14:37.228384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import inflection\n",
    "import string"
   ],
   "id": "9b01970e6cc1af84",
   "outputs": [],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:14:39.927959Z",
     "start_time": "2025-04-12T13:14:39.924670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _split_camelcase(tokens):\n",
    "    # copy token\n",
    "    returning_tokens = tokens[:]\n",
    "    for token in tokens:\n",
    "        split_tokens = re.split(fr'[{string.punctuation}]+', token)\n",
    "        # if token is split into some other tokens\n",
    "        if len(split_tokens) > 1:\n",
    "            returning_tokens.remove(token)\n",
    "            # camelcase defect for new tokens\n",
    "            for st in split_tokens:\n",
    "                camel_split = inflection.underscore(st).split('_')\n",
    "                if len(camel_split) > 1:\n",
    "                    returning_tokens.append(st)\n",
    "                    returning_tokens = returning_tokens + camel_split\n",
    "                else:\n",
    "                    returning_tokens.append(st)\n",
    "        else:\n",
    "            camel_split = inflection.underscore(token).split('_')\n",
    "            if len(camel_split) > 1:\n",
    "                returning_tokens = returning_tokens + camel_split\n",
    "    return returning_tokens"
   ],
   "id": "5f34857803fee81",
   "outputs": [],
   "execution_count": 314
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:16:53.807453Z",
     "start_time": "2025-04-12T13:16:53.802612Z"
    }
   },
   "cell_type": "code",
   "source": "from assets import stop_words",
   "id": "ba2c6abd5f8757b9",
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:21:55.077456Z",
     "start_time": "2025-04-12T13:21:55.072699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocessing_extracted_code_entities(entities):\n",
    "    temp1 = _split_camelcase(entities)\n",
    "    remove_stopwords = [word for word in temp1 if word not in stop_words]\n",
    "    normalize_word = [w.lower() for w in remove_stopwords]\n",
    "    return normalize_word"
   ],
   "id": "965b59b12abf3f32",
   "outputs": [],
   "execution_count": 324
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:21:56.700087Z",
     "start_time": "2025-04-12T13:21:56.680372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for bug_id in extracted_code_entities:\n",
    "    extracted_code_entities[bug_id] = preprocessing_extracted_code_entities(extracted_code_entities[bug_id])"
   ],
   "id": "47bbf158e7c77336",
   "outputs": [],
   "execution_count": 325
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_10_bug_report_dataset",
   "id": "e735719ce18aaf47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_10_bug_report_dataset['code_entities'] = list_10_bug_report_dataset['key'].apply(lambda x: extracted_code_entities[x])",
   "id": "b979aaae406ddca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RERANK",
   "id": "2efdf9cded455b16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_10_bug_report_dataset",
   "id": "617d551d86b33dc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:23:25.975816Z",
     "start_time": "2025-04-12T13:23:25.971255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_final_score_after_llm_evaluate(report):\n",
    "    rvsm_score = compute_rank_files_indexing(report, 0.4)\n",
    "\n",
    "    return dict(sorted(rvsm_score.items(), key=lambda item: item[1], reverse=True))"
   ],
   "id": "94ba15aa46a0592c",
   "outputs": [],
   "execution_count": 329
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "list_prediction_10_files = {}\n",
    "\n",
    "for idx, bug_report in list_10_bug_report_dataset.iterrows():\n",
    "    scores = calculate_final_score_after_llm_evaluate(bug_report)\n",
    "\n",
    "    list_prediction_10_files[bug_report.key] = scores"
   ],
   "id": "c1a6864d94e578b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:23:37.108902Z",
     "start_time": "2025-04-12T13:23:37.102813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy_1 = top_k_accuracy(list_prediction_10_files, first_100_ground_truth, 1)\n",
    "accuracy_5 = top_k_accuracy(list_prediction_10_files, first_100_ground_truth, 5)\n",
    "accuracy_10 = top_k_accuracy(list_prediction_10_files, first_100_ground_truth, 10)\n"
   ],
   "id": "6a60338a991e8309",
   "outputs": [],
   "execution_count": 331
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:23:37.750703Z",
     "start_time": "2025-04-12T13:23:37.746698Z"
    }
   },
   "cell_type": "code",
   "source": "print(accuracy_1, accuracy_5, accuracy_10)",
   "id": "9c1dbff2421934c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.3 0.5\n"
     ]
    }
   ],
   "execution_count": 332
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "list_prediction_10_files",
   "id": "f8bddb9b06908dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ecb0cda476cfe25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
